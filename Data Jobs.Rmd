---
title: "Data Jobs in Salt Lake City"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill

---

```{r resources, include=FALSE}

# Packages
library(flexdashboard)
library(tidyverse)
library(rvest)
library(xml2)
library(digest)
library(DT) 
library(stringr)
library(shiny)

# Searches associated with programs

mscm <- c("cybersecurity analyst", "cybersecurity manager", "cybersecurity consultant",
          "chief information security officer", "director of security", "security systems engineer", 
          "security analyst", "security manager", "security auditor", "security architect",
          "security consultant","information security analyst","network security engineer",
          "information security manager","security compliance analyst","penetration tester",
          "vulnerability assessment analyst")

msis <- c("data engineer", "data architect", "cloud engineer", 
           "data warehouse analyst", "data warehouse engineer",
          "data warehouse architect","business intelligence analyst",
           "database administrator", "data integration engineer" )

msba <- c("data analyst", "data scientist")

# empty data frame to return on html parsing error using tryCatch()
error_df <- data.frame(Search = NA,
                   Date = NA, 
                   Title = NA, 
                   Company = NA, 
                   Location = NA, 
                   Link = NA,
                   Description = NA,
                   ID = NA)
```

```{r scraper, include = F}
job_scraper <- function(position = "data analyst"){  # function can take search phrase with up to 4 words, and at least 2
  
  library(rvest)
  library(xml2)
  library(digest)
  library(tidyverse)
  
  # Format search term
  search <- gsub(pattern = " ", replacement = "%20", x = position)
  
  # Format url
  url <- paste0("https://www.indeed.com/jobs?q=",search,"&l=Salt%20Lake%20City%2C%20UT&radius=50&fromage=2")
  
  # Read HTML
  page <- xml2::read_html(url)
                   
  # Get Job Title
  title <- page %>%
    html_nodes('.jobTitle') %>%
    html_nodes('span') %>%
    html_text('title') 
  
  title <- title[title != "new"]
  
  if(length(title)==0){
    
    df <- data.frame(Search = position,
                   Date = NA, 
                   Title = NA, 
                   Company = NA, 
                   Location = NA, 
                   Link = NA,
                   Description = NA,
                   ID = NA)
    
    return(df)

 
     } else {
  
  # Get Company Name
  company <- page %>%
    html_nodes('.companyName') %>%
    html_text('companyName')
  
  # Get Location
  location <- page %>%
    html_nodes('.companyLocation') %>%
    html_text('companyLocation')
  
  # Get Link
  link <- page %>%
    html_nodes('[data-hide-spinner = "true"]') %>%
    html_attr('href')
  
  link <- paste0("https://www.indeed.com", link) 
  
  # Get Job Descriptions
  job_description <- NA # vector for descriptions
    
  # function to get description from each link
  get_description <- function(link){ 
    page <- xml2::read_html(link)
    description <- page %>%
          html_nodes('.jobsearch-jobDescriptionText') %>%
          html_text()
     ifelse(length(description) > 0, description, NA) # sometimes the description appears to be missing. if so, replace w NA
  }
  
  for(j in seq_along(link)) {
   job_description[j] <- get_description(link[j])
   Sys.sleep(1)
 }
  
  # Get date
  date <- Sys.Date()
  
  # Store jobs in data frame and do cleaning
  
  df <- data.frame(Search = rep(position, length(job_description)),
                   Date = date, 
                   Title = title, 
                   Company = company, 
                   Location = location, 
                   Link = link,
                   Description = job_description) %>% 
    mutate(Description = gsub("[\r\n]", "*", Description), # Clean descriptions
           ID = digest(c(Title, Location, Company, Date, Link))) %>% #Create unique ID
    distinct(Link, ID, .keep_all = T) 
  
  return(df)
  
     }
  
  Sys.sleep(rnorm(1, 8, 2)) # random sleep
  
  # Close open connection
   on.exit(close(url))
}
```

```{r read_data, include = F}

# Write empty job_data.csv once to persistent location

# job_data <- data.frame(Search = "", Date =as.Date(""), Title ="", Company ="", Location ="", Description ="", Link = "", ID = "")

# write_csv(job_data, "/opt/app-data/job_data_test.csv")

data <- read_csv("/opt/app-data/job_data.csv")

```

  
```{r extract, include = F}

# length(mscm)
# length(msis)
# length(msbas)

new_data <- data %>%
  bind_rows(tryCatch(job_scraper(mscm[1]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[2]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[3]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[4]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[5]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[6]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[7]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[8]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[9]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[10]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[11]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[12]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[13]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[14]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[15]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[16]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(mscm[17]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[1]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[2]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[3]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[4]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[5]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[6]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[7]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[8]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msis[9]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msba[1]), error = function(x) return(x = error_df))) %>%
  bind_rows(tryCatch(job_scraper(msba[2]), error = function(x) return(x = error_df))) %>%
  filter(!grepl("United States", Location),
         Company!="CyberCoders") %>%
  distinct(Search, Title, Company, Location, Description, .keep_all = T) %>% 
  select(Search, Date, Title, Company, Location, Description, Link, ID) %>% 
  arrange(desc(Date), Company) %>% 
  na.omit

 write_csv(new_data, "/opt/app-data/job_data.csv")

 
 
```

```{r include=F}
df <- new_data
df$Remote1_Des <-str_detect(df$Description, "remote|Remote|REMOTE|work from home|WORK FROM HOME|work from anywhere")
df$Remote2_Tit <-str_detect(df$Title, "remote|Remote|REMOTE|work from home|WORK FROM HOME|work from anywhere")
df$Remote3_Loca <-str_detect(df$Location, "remote|Remote|REMOTE|work from home|WORK FROM HOME|work from anywhere")

#Then we concatenate the results to compare the results in the three columns in a new column call Concatenate
df$Concatenate <- paste(df$Remote1_Des, df$Remote2_Tit,df$Remote3_Loca)
df$Remote <-str_detect(df$Concatenate, "TRUE")

#If the columns contains at least one TRUE that means the the position is remote, then the rest position will be on-site be default
df$Remote <-with(df, factor(Remote, levels = c("TRUE", "FALSE"), labels = c("Remote", "On-site")))

#At the end we drop all the columns that we create, leaving only one
drop <- c("Remote1_Des","Remote2_Tit","Remote3_Loca","Concatenate")
df = df[,!(names(df) %in% drop)]

```

```{r include=F}
#Creating filter to find only Hybrid positions
#We are looking for the word Hybrid in three different columns were the company may publish this information, 1- Description, 2- Title and 3- Location
df$Hybrid1_Des <-str_detect(df$Description, "hybrid|Hybrid|HYBRID")
df$Hybrid2_Tit <-str_detect(df$Title, "hybrid|Hybrid|HYBRID")
df$Hybrid3_Loca <-str_detect(df$Location, "hybrid|Hybrid|HYBRID")

#Then we concatenate the results to compare the results in the three columns in a new column call Concatenate
df$Concatenate <- paste(df$Hybrid1_Des, df$Hybrid2_Tit,df$Hybrid3_Loca)
df$Hybrid <-str_detect(df$Concatenate, "TRUE")

#If the columns contains at least one TRUE that means the the position is Hybrid, then the rest positions will be false be default
df$Hybrid <-with(df, factor(Hybrid, levels = c("TRUE", "FALSE"), labels = c("Hybrid", "False")))

#At the end we drop all the columns that we create, leaving only one
drop <- c("Hybrid1_Des","Hybrid2_Tit","Hybrid3_Loca","Concatenate")
df = df[,!(names(df) %in% drop)]
```

```{r include=F}
#In order to have only one column with the Work Model that the companies offer we are going to add the Hybrid positions into the column Remote
df$Remote <- as.character(df$Remote)
df$Remote[df$Hybrid == "Hybrid"] <- "Hybrid"

#We drop the column Hybrid
drop <- c("Hybrid")
df = df[,!(names(df) %in% drop)]

#We rename the column remote to Work Model
names(df)[names(df) == 'Remote'] <- 'Work Model'
#head(df)
```

```{r include=F}
#Creating filter Jobtype
df$Job_Type1_Des <-str_detect(df$Description, "Full-time|Full-Time|full-time|FULL-TIME|Fulltime|fulltime|FULLTIME")
df$Job_Type2_Tit <-str_detect(df$Title, "Full-time|Full-Time|full-time|FULL-TIME|Fulltime|fulltime|FULLTIME")
df$Job_Type3_Loca <-str_detect(df$Location, "Full-time|Full-Time|full-time|FULL-TIME|Fulltime|fulltime|FULLTIME")

df$Concatenate <- paste(df$Job_Type1_Des, df$Job_Type2_Tit,df$Job_Type3_Loca)
df$Job_Type <-str_detect(df$Concatenate, "TRUE")
df$Job_Type <-with(df, factor(Job_Type, levels = c("TRUE", "FALSE"), labels = c("Full-time", "Part-time")))

drop <- c("Job_Type1_Des","Job_Type2_Tit","Job_Type3_Loca","Concatenate")
df = df[,!(names(df) %in% drop)]
```

```{r include=F}
#Classifying entries by program
df <- df %>%
  mutate(Program = case_when(
      str_detect(Search, string = "cybersecurity analyst|cybersecurity manager|cybersecurity consultant|chief information security officer|director of security|security systems engineer|security analyst|security manager|security auditor|security architect|security consultant|information security analyst|network security engineer|information security manager|security compliance analyst|penetration tester|vulnerability assessment analyst") ~ "MSCM",
      str_detect(Search, string = "data engineer|data architect|cloud engineer|data warehouse analyst|data warehouse engineer|data warehouse architect|business intelligence analyst|database administrator|data integration engineer") ~ "MSIS",
      str_detect(Search, string = "data analyst|data scientist") ~ "MSBA"))
```

```{r include=F}
#Looking for the skills that the companies require
df$R <-str_detect(df$Description, "RStudio|rstudio|RSTUDIO|R-Studio|r-studio|R-STUDIO|R Studio|r studio|R STUDIO|R ")
df$Python <-str_detect(df$Description, "Python|python|PYTHON")
df$SQL <-str_detect(df$Description, "SQL|sql|Postgresql|POSTGRESQL|postgresql|MySQL|mysql|MYSQL")
df$Tableau <-str_detect(df$Description, "Tableau|tableau|TABLEAU")
df$PowerBI <-str_detect(df$Description, "PowerBI|powerbi|POWERBI")
df$Azure <-str_detect(df$Description, "Azure|azure|AZURE")
df$Excel <-str_detect(df$Description, "Excel|excel|EXCEL")
df$Talend <-str_detect(df$Description, "Talend|talend|TALEND")
df$SAS <-str_detect(df$Description, "SAS|sas")
df$Spark <-str_detect(df$Description, "Spark|spark|SPARK")
df$Qualtrics <-str_detect(df$Description, "Qualtrics|qualtrics|QUALTRICS")
df$Domo <-str_detect(df$Description, "Domo|domo|DOMO")
df$Salesforce <-str_detect(df$Description, "Salesforce|salesforce|SALESFORCE")
df$SAP <-str_detect(df$Description, "SAP|sap")
df$Hadoop <-str_detect(df$Description, "Hadoop|hadoop|HADOOP")
df$Qlik <-str_detect(df$Description, "Qlik|qlik|QLIK")
df$Looker <-str_detect(df$Description, "Looker|looker|LOOKER")
df$Zoho <-str_detect(df$Description, "Zoho|zoho|ZOHO")
df$Stata <-str_detect(df$Description, "Stata|stata|STATA")
df$Snowflake <-str_detect(df$Description, "Snowflake|snowflake|SNOWFLAKE")

df$R = as.integer(as.logical(df$R))
df$Python = as.integer(as.logical(df$Python))
df$SQL = as.integer(as.logical(df$SQL))
df$Tableau = as.integer(as.logical(df$Tableau))
df$PowerBI = as.integer(as.logical(df$PowerBI))
df$Azure = as.integer(as.logical(df$Azure))
df$Excel = as.integer(as.logical(df$Excel))
df$Talend = as.integer(as.logical(df$Talend))
df$SAS = as.integer(as.logical(df$SAS))
df$Spark = as.integer(as.logical(df$Spark))
df$Qualtrics = as.integer(as.logical(df$Qualtrics))
df$Domo = as.integer(as.logical(df$Domo))
df$Salesforce = as.integer(as.logical(df$Salesforce))
df$SAP = as.integer(as.logical(df$SAP))
df$Hadoop = as.integer(as.logical(df$Hadoop))
df$Qlik = as.integer(as.logical(df$Qlik))
df$Looker = as.integer(as.logical(df$Looker))
df$Zoho = as.integer(as.logical(df$Zoho))
df$Stata = as.integer(as.logical(df$Stata))
df$Snowflake = as.integer(as.logical(df$Snowflake))
```

```{r include=F}
#Summing Skills to create a df transposed
Skills <- c("R", "Python","SQL","PowerBI","Azure","Excel","Talend","SAS","Spark","Qualtrics","Domo","Salesforce","SAP","Hadoop","Qlik","Looker","Zoho","Stata","Snowflake")

Skills_count <- c(sum(df$R),
sum(df$Python),
sum(df$SQL),
sum(df$PowerBI),
sum(df$Azure),
sum(df$Excel),
sum(df$Talend),
sum(df$SAS),
sum(df$Spark),
sum(df$Qualtrics),
sum(df$Domo),
sum(df$Salesforce),
sum(df$SAP),
sum(df$Hadoop),
sum(df$Qlik),
sum(df$Looker),
sum(df$Zoho),
sum(df$Stata),
sum(df$Snowflake))



df.skills <- data.frame(Skills, Skills_count)

#df.skills
```

```{r include=F}
#Eliminating ID from API
drop <- c("ID")
df = df[,!(names(df) %in% drop)]

#Creating a new unique ID
df$ID <- paste(df$Title,df$Company,df$Location,df$Date,df$Link)
df$ID <- sapply(df$ID, digest)
#df

#Eliminating duplicate data from ID
df<-df[!duplicated(df$ID),]
```


```{r include=F}
#Writing CSV files
write_csv(df, "/opt/app-data/job_data.csv")
write_csv(df.skills, "/opt/app-data/skillssum.csv")
```

Column {.tabset .tabset-fade}
-------------------------------------

### Open Positions 

```{r}


table_data  <- new_data %>% 
  mutate(Program = ifelse(Search %in% mscm, "MSCM",
                    ifelse(Search %in% msis, "MSIS", "MSBA"))) %>% 
  filter(Date > max(Date) - 60,
         (str_detect(string = tolower(Description), pattern = Search) |
           str_detect(string = tolower(Title), pattern = Search))) %>% # Ensure that search term is actually in the description or the job title
  select(Search, Program, Date, Title, Company, Location, Description, Link, ID) %>% 
  arrange(desc(Date), Company) 
  

table_data$Title <- paste0("<a href='", table_data$Link,"'>", table_data$Title,"</a>")


  
  datatable(select(table_data, -Link, -ID),
              # fillContainer = T,
              escape=F, # This is essential for link display
              extensions = 'Buttons',
              options = list(autoWidth = TRUE,
                             rownames = F,
                             dom = 'Bfrtip',
                             buttons = c('csv', 'excel'),
                             lengthMenu = list(c(10, 25, 50, -1), c(10, 25, 50, "All")),
                             columnDefs = list(
                               list(
                                 targets = c(7), #ie, Description column for hover
                                 render = JS(
                                       "function(data, type, row, meta) {",
                                       "return type === 'display' && data.length > 360 ?",
                                       "'<span title=\"' + data + '\">' + data.substr(0, 360) + '...</span>' : data;",
                                       "}"))),
                             scrollY=F), # This allows for proper  vertical sizing
              callback = JS('table.page(3).draw(false);'),
              caption = "Table includes jobs from the previous 60 days. Hover cursor over Description for full text. Use the search box to find specific job titles or positions  associated with programs. Use the above buttons to download the entire table.")  %>%
  formatStyle(c("Search","Program","Date", "Title", "Company", "Location", "Description"), "vertical-align"="top")




```
   
### Job Search Terms by Program

**MSBA**: 

- data analyst
- data scientist

**MSCM**: 

- cybersecurity analyst
- cybersecurity manager
- cybersecurity consultant
- chief information security officer
- director of security
- security systems engineer
- security analyst
- security manager
- security auditor
- security architect
- security consultant
- information security analyst
- network security engineer
- information security manager
- security compliance analyst
- penetration tester
- vulnerability assessment analyst

**MSIS**: 

- data engineer
- data architect
- cloud engineer
- data warehouse analyst
- data warehouse engineer
- data warehouse architect
- business intelligence analyst
- database administrator
- data integration engineer

### Postings


```{r}
library(ggplot2)

new_data %>% 
  filter(str_detect(string = tolower(Description), pattern = Search) |
           str_detect(string = tolower(Title), pattern = Search)) %>% 
  group_by(Date) %>% 
  count() %>% 
  ggplot(aes(Date, n))+
  geom_line()+
  theme_minimal()+
  labs(title = "Count of Job Postings by Date",
       y = "count",
       caption = "Note: additional search  terms  for MSIS and MSCM programs added on 2/17/22")
  
  
```




### Text Analysis 

```{r}
library(stringr)

new_data$Description <- gsub('[[:punct:]]', ' ', new_data$Description)

# new_data %>% 
#   group_by(Date, ID) %>% 
#   summarize(R = sum(str_detect(string = Description, pattern = " R ")),
#             Python = sum(str_detect(string = Description, pattern = " Python ") |
#                           str_detect(string = Description, pattern = " python ") ),
#             `R only` = ifelse((R > 0 & Python ==0), R, 0),
#             `Python only` = ifelse((R == 0 & Python > 0), Python, 0)) %>% 
#   group_by(Date) %>% 
#   summarize(R = sum(R),
#             Python = sum(Python),
#          `R only` = sum(`R only`),
#          `Python only` = sum(`Python only`)) %>% 
#   mutate(R = cumsum(R),
#             Python = cumsum(Python),
#          `R only` = cumsum(`R only`),
#          `Python only` = cumsum(`Python only`)) %>% 
#   pivot_longer(cols = c("R", "Python", "R only", "Python only"), 
#                names_to = "Language", 
#                values_to = "count") %>% 
#   ggplot(aes(Date, count, col=Language))+
#   geom_line()+
#   theme_minimal()+
#   labs(title = "Cumulative Mentions of R vs. Python in Job Descriptions",
#        y = "count")

new_data %>% 
  filter(str_detect(string = tolower(Description), pattern = Search) |
           str_detect(string = tolower(Title), pattern = Search)) %>% 
  group_by(Date, ID) %>% 
  summarize(R = sum(str_detect(string = Description, pattern = " R ")),
            Python = sum(str_detect(string = Description, pattern = " Python ") |
                          str_detect(string = Description, pattern = " python "))) %>% 
  group_by(Date) %>% 
  summarize(R = sum(R),
            Python = sum(Python)) %>% 
  mutate(R = cumsum(R),
            Python = cumsum(Python)) %>% 
  pivot_longer(cols = c("R", "Python"), 
               names_to = "Language", 
               values_to = "count") %>% 
  ggplot(aes(Date, count, col=Language))+
  geom_line()+
  theme_minimal()+
  labs(title = "Cumulative Mentions of R vs. Python in Job Descriptions",
       y = "count")


```


  
### Job Description Topics

```{r}

```



    





